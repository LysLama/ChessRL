# Chess Reinforcement Learning Project

Dá»± Ã¡n nghiÃªn cá»©u vÃ  phÃ¡t triá»ƒn há»‡ thá»‘ng há»c tÄƒng cÆ°á»ng (RL) cho cá» vua, táº­p trung vÃ o tÃ­nh module hÃ³a, kháº£ nÄƒng kiá»ƒm thá»­ vÃ  tÃ¡i táº¡o káº¿t quáº£.

## ğŸš€ Roadmap tá»•ng quan

Dá»± Ã¡n Ä‘Æ°á»£c phÃ¢n chia thÃ nh hai giai Ä‘oáº¡n lá»›n:
1. **3 tuáº§n triá»ƒn khai codebase** - XÃ¢y dá»±ng ná»n táº£ng á»•n Ä‘á»‹nh
2. **5 tuáº§n huáº¥n luyá»‡n vÃ  thá»­ nghiá»‡m** - Thá»±c hiá»‡n cÃ¡c thÃ­ nghiá»‡m vÃ  Ä‘Ã¡nh giÃ¡

## ğŸ¯ 3-Week Implementation Roadmap (Code & Infrastructure)

Má»¥c tiÃªu: XÃ¢y dá»±ng codebase module, Ä‘Æ°á»£c kiá»ƒm thá»­ vÃ  tÃ¡i táº¡o Ä‘Æ°á»£c káº¿t quáº£ vá»›i training loop hoÃ n chá»‰nh cho cÃ¡c thá»­ nghiá»‡m nhanh.

### Tuáº§n 1 â€” á»”n Ä‘á»‹nh mÃ´i trÆ°á»ng vÃ  core game 
**Focus**: LÃ m vá»¯ng cháº¯c cÃ¡c module game/ vÃ  environments/ Ä‘á»ƒ code RL cÃ³ thá»ƒ dá»±a vÃ o giao diá»‡n cÃ³ tÃ­nh xÃ¡c Ä‘á»‹nh.

**Tasks**:
1. **TiÃªu chuáº©n hÃ³a Board API** (game/chess_board.py, chinese_chess_board.py)
   - Methods: reset(), get_legal_moves(player) -> List[Move], apply_move(move) -> reward, done, is_game_over(), to_observation()
   - Biá»ƒu diá»…n state: tensor sá»‘ (e.g., shape (N_planes, H, W)) vÃ  má»™t biá»ƒu diá»…n nhá» gá»n Ä‘á»ƒ hashing
2. **Äá»‘i tÆ°á»£ng Move** (game/utils.py hoáº·c game/move.py)
   - Immutable dataclass chá»©a (from_sq, to_sq, promotion, metadata)
3. **Unit tests cho tÃ­nh Ä‘Ãºng Ä‘áº¯n luáº­t chÆ¡i** (nÆ°á»›c Ä‘i cÆ¡ báº£n, báº¯t quÃ¢n, phÃ¡t hiá»‡n chiáº¿u/chiáº¿u háº¿t)
4. **Triá»ƒn khai base_env.py** vá»›i reset(), step(action), render(mode='human'|'rgb_array')
5. **Triá»ƒn khai chess_env.py** (wrap board; chuyá»ƒn Ä‘á»•i move index â†” Ä‘á»‘i tÆ°á»£ng Move)

**Deliverables**:
- Unit tests cho logic board (Ã­t nháº¥t 25 assertions covering cÃ¡c trÆ°á»ng há»£p biÃªn)
- ChessEnv tÆ°Æ¡ng thÃ­ch Gym vá»›i hÃ nh vi seed() cÃ³ tÃ­nh xÃ¡c Ä‘á»‹nh

**Validation**:
- python -m pytest tests/test_board.py passes
- env.step(action) táº¡o ra cÃ¹ng state khi cÃ¹ng seed vÃ  chuá»—i hÃ nh Ä‘á»™ng

### Tuáº§n 2 â€” Baseline agents + Utilities
**Focus**: LÃ m cho agents cÃ³ tÃ­nh plug-n-play vÃ  thÃªm cÃ¡c agents cÆ¡ báº£n cÃ¹ng cÃ´ng cá»¥ (replay buffer, logging).

**Tasks**:
1. **HoÃ n thiá»‡n API agents/base_agent.py**
   - Methods: select_action(obs), train_step(batch) (optional), save(path), load(path)
2. **Triá»ƒn khai Random agent vÃ  Q-learning (tabular)** cÃ³ thá»ƒ cháº¡y end-to-end
3. **Replay buffer & training/replay_buffer** náº¿u sá»­ dá»¥ng DQN sau nÃ y
4. **models/networks.py**: khung ConvNet/MLP nhá» vá»›i forward(obs)
5. **Khung Trainer** (training/trainer.py) cÃ³ thá»ƒ:
   - Cháº¥p nháº­n env, agent, config
   - Cháº¡y episodes, ghi log rewards, checkpoint models má»—i N episodes
6. **Logging**: tÃ­ch há»£p utils/logger.py (console + CSV + TensorBoard tÃ¹y chá»n)
7. **Script thá»­ nghiá»‡m cÆ¡ báº£n** (training/experiments.py) Ä‘á»ƒ cháº¡y:
   - python -m training.experiments --agent random --env chess

**Deliverables**:
- Random vÃ  tabular Q agents cháº¡y ~100 episodes
- Logs vÃ  checkpoints Ä‘Æ°á»£c lÆ°u

**Validation**:
- Trainer cháº¡y 100 episodes khÃ´ng crash; logs hiá»ƒn thá»‹ reward má»—i episode

### Tuáº§n 3 â€” Deep agents, tests, vÃ  CI
**Focus**: Triá»ƒn khai DQN, tests, config, vÃ  tÃ­nh tÃ¡i táº¡o káº¿t quáº£.

**Tasks**:
1. **DQN agent** (agents/dqn_agent.py)
   - Prioritized replay tÃ¹y chá»n nhÆ°ng báº¯t Ä‘áº§u vá»›i uniform
   - Target network, epsilon-greedy schedule, gradient clipping
2. **Khung PPO** (agents/ppo_agent.py) â€” chá»‰ cáº§n giao diá»‡n cho giai Ä‘oáº¡n huáº¥n luyá»‡n sau
3. **Model save/load** (models/model_utils.py) vÃ  cáº¥u trÃºc models/saved_models
4. **Pipeline Ä‘Ã¡nh giÃ¡** (training/evaluator.py) Ä‘á»ƒ tÃ­nh win-rate, avg-reward, Ä‘á»™ dÃ i episode trung bÃ¬nh
5. **Unit tests** cho:
   - TÃ­nh Ä‘Ãºng Ä‘áº¯n cá»§a Replay buffer (cÃ¡c shapes khi sampling),
   - Shapes cá»§a DQN forward pass,
   - Trainer checkpointing
6. **CI hook** (GitHub Actions hoáº·c script local) Ä‘á»ƒ cháº¡y tests khi push

**Deliverables**:
- DQN training cháº¡y cho smoke-test ngáº¯n (vÃ­ dá»¥ 500 steps)
- CI cháº¡y pytest vÃ  kiá»ƒm tra lint

**Validation**:
- pytest pass cho cÃ¡c tests má»›i
- DQN agent cÃ³ thá»ƒ há»c cáº¥u trÃºc reward Ä‘Æ¡n giáº£n (sanity check)

## ï¿½ 5-Week Training Roadmap (Experiments, Tuning, Evaluation)

Má»¥c tiÃªu: Táº¡o ra vÃ  so sÃ¡nh cÃ¡c agents (Random, Q-Learning, DQN, PPO) vÃ  chá»n model cuá»‘i cÃ¹ng vá»›i bÃ¡o cÃ¡o Ä‘Ã¡nh giÃ¡.

### Tuáº§n 0 (chuáº©n bá»‹) â€” Compute & baseline metrics (trÃ¹ng vá»›i káº¿t thÃºc tuáº§n 3)
- Chuáº©n bá»‹ compute: GPU local hoáº·c cloud (chá»‰ rÃµ loáº¡i GPU), ghi láº¡i RAM, GPU memory
- Thiáº¿t láº­p metrics cÆ¡ sá»Ÿ: tá»· lá»‡ tháº¯ng cá»§a random agent, hiá»‡u suáº¥t Q-learning
- XÃ¡c Ä‘á»‹nh cÃ¡c tráº­n Ä‘Ã¡nh giÃ¡: 1000 episodes self-play vÃ  vs Random agent

### Tuáº§n 4 (Training Week 1) â€” DQN baseline run
**Tasks**:
1. **Hyperparam sweep (coarse)**: learning rate [1e-4, 1e-3], batch size [32, 128], gamma [0.99]
2. **Cháº¡y 3 DQN runs** (seeds khÃ¡c nhau) cho ~100k environment steps (hoáº·c giá»›i háº¡n thá»i gian)
3. **Theo dÃµi** reward má»—i episode, moving average, loss curves, vÃ  evaluation snapshots má»—i 5k steps

**Deliverables**:
- 3 DQN runs vá»›i logs, saved checkpoints táº¡i cÃ¡c Ä‘iá»ƒm
- Plots cÃ¡c learning curves

**Metrics**:
- Cáº£i thiá»‡n moving average reward so vá»›i random; sample win rate vs random má»—i checkpoint

### Tuáº§n 5 (Training Week 2) â€” DQN tuning & robustness
**Tasks**:
1. **Fine-tune hyperparameters tá»‘t nháº¥t** tá»« tuáº§n 4
2. **Cháº¡y dÃ i hÆ¡n** (e.g., 300k steps) cho config tá»‘t nháº¥t
3. **Ablation**: target update frequency, replay buffer size, network depth

**Deliverables**:
- Má»™t checkpoint "best DQN" + bÃ¡o cÃ¡o ablation

**Metrics**:
- Win rate vs random vÃ  vs Q-learning baseline; stability across seeds

### Tuáº§n 6 (Training Week 3) â€” PPO experiments
**Tasks**:
1. **Triá»ƒn khai PPO** Ä‘áº§y Ä‘á»§ náº¿u khung chÆ°a xong: actor-critic network, GAE, clip ratio
2. **Cháº¡y PPO** vá»›i hyperparams ban Ä‘áº§u: learning rate 3e-4, batch size 2048 (hoáº·c Ä‘iá»u chá»‰nh), epochs má»—i update 10
3. **So sÃ¡nh PPO vs DQN** trÃªn cÃ¹ng ngÃ¢n sÃ¡ch compute

**Deliverables**:
- PPO run logs vÃ  checkpoint tá»‘t nháº¥t

**Metrics**:
- Sample efficiency (hiá»‡u suáº¥t má»—i training step), final win rates, stability

### Tuáº§n 7 (Training Week 4) â€” Model comparison & ensemble/self-play
**Tasks**:
1. **Giáº£i Ä‘áº¥u head-to-head**: DQN vs PPO vs Q-learning vs Random (round-robin)
2. **Self-play training runs** náº¿u Ã¡p dá»¥ng Ä‘Æ°á»£c (train via self-play Ä‘á»ƒ cáº£i thiá»‡n)
3. **Thu tháº­p metrics**: xáº¿p háº¡ng kiá»ƒu Elo, tá»· lá»‡ tháº¯ng head-to-head, Ä‘á»™ dÃ i episode trung bÃ¬nh, cÃ¡c dáº¡ng tháº¥t báº¡i rÃµ rÃ ng

**Deliverables**:
- Káº¿t quáº£ giáº£i Ä‘áº¥u, scripts Ä‘á»ƒ tÃ¡i táº¡o cÃ¡c tráº­n Ä‘áº¥u

**Metrics**:
- Xáº¿p háº¡ng Elo, kiá»ƒm tra Ã½ nghÄ©a thá»‘ng kÃª (vÃ­ dá»¥ bootstrap win-rate CI)

### Tuáº§n 8 (Training Week 5) â€” Final evaluation, report & checkpoints
**Tasks**:
1. **HoÃ n thiá»‡n model tá»‘t nháº¥t** vÃ  freeze hyperparams
2. **ÄÃ¡nh giÃ¡ Ä‘áº§y Ä‘á»§**: 10k games vs baselines, record logs vÃ  lÆ°u final model
3. **Táº¡o bÃ¡o cÃ¡o cuá»‘i cÃ¹ng**:
   - Training curves, hyperparams, báº£ng Ä‘Ã¡nh giÃ¡, cÃ¡c trÆ°á»ng há»£p tháº¥t báº¡i, cÃ¡c bÆ°á»›c tiáº¿p theo Ä‘Æ°á»£c Ä‘á» xuáº¥t
4. **Tá»•ng káº¿t**: thÃªm README Ä‘á»ƒ tÃ¡i táº¡o thá»­ nghiá»‡m, model cards cho saved models

**Deliverables**:
- Final model files trong models/saved_models/, bÃ¡o cÃ¡o Ä‘Ã¡nh giÃ¡ (Markdown + plots), reproduction script

**Metrics / Acceptance criteria**:
- Äáº¡t threshold tá»· lá»‡ tháº¯ng mong muá»‘n vs random/baselines (Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh dá»±a trÃªn hiá»‡u suáº¥t cháº¥p nháº­n Ä‘Æ°á»£c)
- Káº¿t quáº£ tÃ¡i táº¡o Ä‘Æ°á»£c (cÃ¹ng seed -> Ä‘Æ°á»ng cong tÆ°Æ¡ng tá»±) trong giá»›i háº¡n Ä‘á»™ biáº¿n thiÃªn

## ğŸ” PhÃ¢n tÃ­ch rá»§i ro & Giáº£m thiá»ƒu

| Rá»§i ro | Giáº£m thiá»ƒu |
|--------|------------|
| **Biá»ƒu diá»…n state lossy / khÃ´ng tÆ°Æ¡ng thÃ­ch giá»¯a cÃ¡c module** | XÃ¡c Ä‘á»‹nh format observation chuáº©n trong game/board.py vÃ  thá»±c thi trong tests |
| **Action space phÃ¡t ná»• (sá»‘ lÆ°á»£ng actions rá»i ráº¡c lá»›n)** | MÃ£ hÃ³a moves gá»n gÃ ng (from,to,promote) vÃ  sá»­ dá»¥ng action masking trong env Ä‘á»ƒ trÃ¡nh illegal action gradient updates |
| **Training khÃ´ng á»•n Ä‘á»‹nh / hiá»‡u quáº£ máº«u tháº¥p** | Báº¯t Ä‘áº§u vá»›i cÃ¡c nhiá»‡m vá»¥ Ä‘Æ¡n giáº£n / biáº¿n thá»ƒ board nhá» hÆ¡n (quy táº¯c Ä‘Æ¡n giáº£n) Ä‘á»ƒ xÃ¡c thá»±c há»c; sá»­ dá»¥ng curriculum/self-play |
| **Pygame/UI lÃ m rá»‘i training vÃ  ngÄƒn headless runs** | LÃ m cho rendering tÃ¹y chá»n vÃ  khÃ´ng bao giá» gá»i bÃªn trong env.step() máº·c Ä‘á»‹nh |
| **Thiáº¿u compute (GPUs)** | Æ¯u tiÃªn cÃ¡c agents nháº¹ (tabular/Q) vÃ  test DQN trÃªn networks nhá» trÆ°á»›c khi má»Ÿ rá»™ng |

## ğŸ› ï¸ Äá» xuáº¥t thá»±c táº¿ vÃ  config

- **Observation tensor**: shape=(N_planes=12, H=8, W=8) (one-hot per piece type per color), dtype float32
- **Action encoding**: map legal moves to indices má»—i step; tráº£ vá» action_mask trong step() náº¿u cÃ³ thá»ƒ
- **Logging**: sá»­ dá»¥ng tensorboardX hoáº·c torch.utils.tensorboard, cá»™ng vá»›i backup CSV trong logs/
- **Checkpoint cadence**: má»—i 5k environment steps hoáº·c má»—i 100 episodes (tÃ¹y cÃ¡i nÃ o nhá» hÆ¡n)

## ğŸ—‚ï¸ Cáº¥u trÃºc Project Má»›i

```
chess_rl/
â”œâ”€â”€ game/                      # Game logic core
â”‚   â”œâ”€â”€ chess_board.py         # Chess rules & board representation  
â”‚   â”œâ”€â”€ chinese_chess_board.py # Optional variant
â”‚   â””â”€â”€ move.py                # Move object implementation
â”œâ”€â”€ environments/              # RL environment wrappers
â”‚   â”œâ”€â”€ base_env.py            # Base environment class
â”‚   â””â”€â”€ chess_env.py           # Chess-specific environment
â”œâ”€â”€ agents/                    # Agent implementations
â”‚   â”œâ”€â”€ base_agent.py          # Abstract base class
â”‚   â”œâ”€â”€ random_agent.py        # Random baseline
â”‚   â”œâ”€â”€ tabular_q_agent.py     # Tabular Q-learning
â”‚   â”œâ”€â”€ dqn_agent.py           # Deep Q-Network
â”‚   â””â”€â”€ ppo_agent.py           # PPO implementation
â”œâ”€â”€ models/                    # Neural network models
â”‚   â”œâ”€â”€ networks.py            # Network architectures
â”‚   â”œâ”€â”€ model_utils.py         # Save/load functionality
â”‚   â””â”€â”€ saved_models/          # Checkpoints directory
â”œâ”€â”€ training/                  # Training infrastructure
â”‚   â”œâ”€â”€ trainer.py             # Main training loop
â”‚   â”œâ”€â”€ replay_buffer.py       # Experience replay
â”‚   â”œâ”€â”€ experiments.py         # Experiment runner
â”‚   â””â”€â”€ evaluator.py           # Evaluation pipeline
â”œâ”€â”€ utils/                     # Utility functions
â”‚   â”œâ”€â”€ logger.py              # Logging utilities
â”‚   â””â”€â”€ visualization.py       # Plotting & visualization
â”œâ”€â”€ tests/                     # Unit & integration tests
â”‚   â”œâ”€â”€ test_board.py          # Board logic tests
â”‚   â”œâ”€â”€ test_env.py            # Environment tests
â”‚   â””â”€â”€ test_agents.py         # Agent implementation tests
â”œâ”€â”€ configs/                   # Configuration files
â”‚   â”œâ”€â”€ default.yaml           # Default parameters
â”‚   â””â”€â”€ experiments/           # Experiment-specific configs
â”œâ”€â”€ logs/                      # Training logs
â”œâ”€â”€ scripts/                   # Utility scripts
â”‚   â””â”€â”€ generate_data.py       # Generate training data
â”œâ”€â”€ notebooks/                 # Analysis notebooks
â”œâ”€â”€ main.py                    # Entry point
â”œâ”€â”€ requirements.txt           # Dependencies
â””â”€â”€ README.md                  # Documentation
```

## âš™ï¸ MÃ´i trÆ°á»ng phÃ¡t triá»ƒn 

### YÃªu cáº§u pháº§n má»m
- Python 3.8+ 
- PyTorch 2.0+ (vá»›i CUDA support cho GPU training)
- Gym / Gymnasium
- pytest
- TensorBoard

### CÃ i Ä‘áº·t mÃ´i trÆ°á»ng
```bash
# Táº¡o vÃ  kÃ­ch hoáº¡t conda environment
conda create -n chess-rl python=3.10
conda activate chess-rl

# CÃ i Ä‘áº·t dependencies
pip install -r requirements.txt

# Kiá»ƒm tra cÃ i Ä‘áº·t 
python -c "import torch; print(f'PyTorch {torch.__version__}, CUDA available: {torch.cuda.is_available()}')"
python -m pytest tests/
```

### PhÃ¡t triá»ƒn vÃ  testing
```bash
# Cháº¡y unit tests
python -m pytest tests/

# Cháº¡y linting
flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics

# Cháº¡y format code
black .
```

## ğŸ“š Tham kháº£o vÃ  resources

### Tiáº¿p cáº­n codebase
1. Äá»c qua mÃ´i trÆ°á»ng trong environments/chess_env.py
2. Hiá»ƒu cÃ¡ch biá»ƒu diá»…n state vÃ  action trong game/chess_board.py
3. Xem cÃ¡c agents cÆ¡ báº£n trong agents/random_agent.py
4. TÃ¬m hiá»ƒu training loop trong training/trainer.py

### TÃ i liá»‡u há»c tÄƒng cÆ°á»ng
- "Reinforcement Learning: An Introduction" bá»Ÿi Sutton & Barto
- PPO Paper: "Proximal Policy Optimization Algorithms" bá»Ÿi Schulman et al.
- DQN Paper: "Human-level control through deep reinforcement learning" bá»Ÿi Mnih et al.
- AlphaZero Paper: "Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm" bá»Ÿi Silver et al.

---

## ğŸ“ LiÃªn há»‡ vÃ  ÄÃ³ng gÃ³p

- **Issues**: BÃ¡o bugs vÃ  feature requests
- **Pull Requests**: ÄÃ³ng gÃ³p code vÃ  improvements
- **Documentation**: Cáº£i thiá»‡n docs vÃ  examples
- **Testing**: ThÃªm test cases vÃ  validation scenarios

## ğŸ“„ License

MIT License - Tá»± do sá»­ dá»¥ng cho nghiÃªn cá»©u vÃ  phÃ¡t triá»ƒn.

---

**Happy Chess AI Development! ğŸ**